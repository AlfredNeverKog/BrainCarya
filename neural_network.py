import numpy as np
import tensorflow as tf


class NeuralNetwork(object):

    @staticmethod
    def generate_layer(size, activation_func):
        """
        Generate Layer info
        size - number of layers
        activation_func - Activation Function, if not presented - layer not use Activation Func
        """
        return {'size': size, 'activation_func': activation_func}

    def __init__(self, hiden_layers, input ,output,session):
        """
        All params should be generated by `generate_layer` func
        :param hiden_layers: Hidden layers
        :param input: Input Layer
        :param output: Output Layer
        :param session: Tensorflow Session
        """
        self.sess = session
        self.input_size = input['size']
        self.output_size = output['size']
        self.layers_info = []

        input_layer = {'weights' : tf.Variable(tf.random_normal([input['size']
                                                              ,hiden_layers[0]['size']])),
                        'biases' : tf.Variable(tf.random_normal([hiden_layers[0]['size']])),
                       'activation_func':input['activation_func']}
        self.layers_info.append(input_layer)

        for index in range(0,len(hiden_layers)):
            prev_hl = self.layers_info[- 1]
            hl      = hiden_layers[index]

            prev_hl_size = prev_hl['weights'].get_shape()
            print('prev shape',prev_hl_size[0].value,prev_hl_size[1].value)

            l = {'weights' : tf.Variable(tf.random_normal([prev_hl_size[1].value,hl['size']])),
                 'biases' : tf.Variable(tf.random_normal([hl['size']])),
                 'activation_func': hl['activation_func']}
            self.layers_info.append(l)

        output_layer = {'weights': tf.Variable(tf.random_normal([hiden_layers[-1]['size']
                                                                   ,output['size'] ])),
                       'biases': tf.Variable(tf.random_normal([hiden_layers[-1]['size']])),
                       'activation_func': output['activation_func']}
        self.layers_info.append(output_layer)
        self.sess.run(tf.initialize_all_variables())

    def proceed_activation(self,layer_info,layer):
        """

        :param layer_info: Layer info generated by generate_layer func
        :param layer: Layer
        :return:
        """
        if 'activation_func' in layer_info:
            layer = layer_info['activation_func'](layer)
        return layer


    def create_layers(self,input):
        """
         Create Array of layers for current network
        :param input: Input data
        :return: Layers with data
        """
        layers = []
        layers.append(self.proceed_activation( self.layers_info[0],tf.matmul(input,self.layers_info[0]['weights'])
                                              + self.layers_info[0]['biases']))
        for info_l_index in range(2,len(self.layers_info) - 1 ):
            info_l      = self.layers_info[info_l_index]

            #print ('prev',info_l_prev['weights'].get_shape())
            #print ('curr', info_l['weights'].get_shape())
            l = tf.matmul(layers[-1],info_l['weights']) + info_l['biases']
            l = self.proceed_activation(info_l,l)
            layers.append(l)

        layers.append(tf.matmul(layers[-1],self.layers_info[-1]['weights']))
        return layers


    def train(self,X,Y,epochs):
        """

        :param X: Input, data shape should be [NUMBER_OF_EXAMPLES, {InputLayerSize}]
        :param Y: Result, shape should be [NUMBER_OF_RESULT,OUTPUT_SIZE]
        :param epochs: Network learns EPOCHs tims
        :return:
        """
        layers = self.create_layers(tf.constant(X,dtype=tf.float32))
        predicated = layers[-1]
        loss = tf.reduce_mean(tf.square(predicated - Y))
        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

        for i in range(epochs):
            self.sess.run(optimizer)
            if i % 1000 == 0:
                print(self.sess.run(loss))

